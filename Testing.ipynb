{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud.storage import Blob\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "def DownloadWeights(modelname,bucket):\n",
    "    '''Downloads all of the model dependencies from Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        modelname (string) - Where in google cloud a particular models dependencies are stored.\n",
    "        bucket (obj) - Which bucket is it in?\n",
    "    Returns:\n",
    "        Indx2Word (dict) - A mapping between numerical indicies and words (or characters).\n",
    "        Word2Indx (dict) - A mapping between words (or characters) to a numerical index.\n",
    "        Language2Indx (dict) - A mapping between languages to a numerical index.\n",
    "        Indx2Language (dict) - An inverse of ^.\n",
    "    '''\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('...')\n",
    "    \n",
    "    blob = bucket.blob('models/nlp/language_classification/{}/lexicons/Indx2Word.json'.format(modelname))\n",
    "    Indx2Word = json.loads(blob.download_as_string().decode())\n",
    "    \n",
    "    blob = bucket.blob('models/nlp/language_classification/{}/lexicons/Word2Indx.json'.format(modelname))\n",
    "    Word2Indx = json.loads(blob.download_as_string().decode())\n",
    "    \n",
    "    blob = bucket.blob('models/nlp/language_classification/{}/lexicons/Language2Indx.json'.format(modelname))\n",
    "    Language2Indx = json.loads(blob.download_as_string().decode())\n",
    "    \n",
    "    blob = bucket.blob('models/nlp/language_classification/{}/lexicons/Indx2Language.json'.format(modelname))\n",
    "    Indx2Language = json.loads(blob.download_as_string().decode())\n",
    "    \n",
    "    blob = bucket.blob('models/nlp/language_classification/{}/model_weights/Weights.h5'.format(modelname))\n",
    "    blob.download_to_filename('Weights.h5')\n",
    "        \n",
    "    return Indx2Word,Word2Indx,Language2Indx,Indx2Language\n",
    "    \n",
    "def CreateMatricies(data, Word2Indx, sequence_length):\n",
    "    '''Creates a numerical representation (matrix) for our source words (or characters).\n",
    "\n",
    "    Args:\n",
    "        data (list) - A list of strings.\n",
    "        Word2Indx (dict) - A mapping between words (or characters) to a numerical index.\n",
    "        sequence_length (int) - How many tokens in a sequence?\n",
    "    Returns:\n",
    "        X (matrix) - A matrix containing the data needed by Keras/TF.\n",
    "        '''\n",
    "    X = []\n",
    "    for row in data:\n",
    "        sentence = [1]\n",
    "        row = row.strip()\n",
    "        for word in row:\n",
    "            word = word.lower()\n",
    "            if word in Word2Indx:\n",
    "                sentence.append(Word2Indx[word])\n",
    "            else:\n",
    "                sentence.append(Word2Indx['<unk>'])\n",
    "        sentence.append(Word2Indx['<end>'])\n",
    "        X.append(sentence)\n",
    "    X = pad_sequences(X,sequence_length,padding='post',truncating='post')\n",
    "    for row_idx, row in enumerate(X):\n",
    "        if 2 not in row:\n",
    "            X[row_idx][-1] = 2\n",
    "    return X\n",
    "    \n",
    "\n",
    "def ReturnModel(Word2Indx, Language2Indx, sequence_length, Embedding_Dim, Neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(Word2Indx), Embedding_Dim, input_length = sequence_length))\n",
    "    model.add(Bidirectional(LSTM(Neurons)))\n",
    "    model.add(Dense(len(Language2Indx), activation='softmax'))\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def PredictBatch(data, threshold = 0.3, sequence_length = 50):\n",
    "    '''Makes a batch prediction on an array of strings. Returns all (multiple languages given the nature of the text )\n",
    "    of the languages where p > threshold.\n",
    "    \n",
    "    Args:\n",
    "        data (list) - List of strings you want to classify\n",
    "        threshold (float) - Probablity for language detection cut off\n",
    "        sequence_length (int) - How many characters/words are each sequence padded to?\n",
    "    Returns:\n",
    "        predictions (pandas Series) - A pandas series containing the predicted languages per string in data.\n",
    "    '''\n",
    "    batch = CreateMatricies(data,Word2Indx,sequence_length)\n",
    "    results = model.predict(batch)\n",
    "    indicies = [np.where(r > threshold)[0] + 1 if sum(r > threshold) > 0  else '0' for r in results]\n",
    "    predictions = [[Indx2Language[str(indx)] for indx in row] for row in indicies]\n",
    "    return pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fas...\n",
      "Starting spa...\n",
      "Starting por...\n",
      "Starting zho...\n",
      "Starting ell...\n",
      "Starting ara...\n",
      "Starting rus...\n",
      "Starting ukr...\n",
      "Starting kor...\n",
      "Starting eng...\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.bucket('...')\n",
    "\n",
    "Lookups = {'Afghanistan/json': 'fas',\n",
    "           'Argentina/json':'spa',\n",
    "           'Brazil/json':'por',\n",
    "           'China/json':'zho',\n",
    "           'Greece/json':'ell',\n",
    "           'Qatar/json':'ara',\n",
    "           'Russia/json':'rus',\n",
    "           'Ukraine/json':'ukr',\n",
    "           'North Korea/json':'kor',\n",
    "           'USA/json':'eng'\n",
    "}\n",
    "Names = set()\n",
    "X = []\n",
    "Y = []\n",
    "for kk,vv in Lookups.items():\n",
    "    print('Starting {}...'.format(vv))\n",
    "    count = 0\n",
    "    while count < 100:\n",
    "        for blob in bucket.list_blobs(prefix=kk):\n",
    "            if count > 100:\n",
    "                break\n",
    "            data = json.loads(blob.download_as_string().decode())\n",
    "            for d in data:\n",
    "                Name = d['EntityName'].strip()\n",
    "                if Name!='' and Name not in Names:\n",
    "                    count = count + 1\n",
    "                    X.append(Name)\n",
    "                    Y.append(vv)\n",
    "                    Names.add(Name)\n",
    "                    if count > 100:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time 8.767\n",
      "Average Rate 115.201\n",
      "Average Accuracy 0.985\n"
     ]
    }
   ],
   "source": [
    "modelname = 'OneBidirectionalLSTM_Char_2019-04-12_128_50_250_512_0.5'\n",
    "\n",
    "sequence_length = 50\n",
    "Embedding_Dim = 250\n",
    "Neurons = 512\n",
    "\n",
    "Indx2Word,Word2Indx,Language2Indx,Indx2Language = DownloadWeights(modelname,bucket)\n",
    "Indx2Language['0'] = 'Unknown'\n",
    "model =  ReturnModel(Word2Indx, Language2Indx, sequence_length, Embedding_Dim, Neurons)\n",
    "model.load_weights('Weights.h5')\n",
    "\n",
    "Times = []\n",
    "while len(Times)<10:\n",
    "    t0 = time.time()\n",
    "    Yhat = PredictBatch(X, threshold = 0.3, sequence_length = 50)\n",
    "    t1 = time.time()\n",
    "    Times.append(t1-t0)\n",
    "    \n",
    "Accuracy = []\n",
    "for index,pred in enumerate(Yhat):\n",
    "    if len(pred)==1:\n",
    "        Accuracy.append(pred == Yhat[index])\n",
    "    else:\n",
    "        acc = False\n",
    "        for p in pred:\n",
    "            if p == Yhat[index]:\n",
    "                acc = True\n",
    "        Accuracy.append(acc)\n",
    "\n",
    "AvgTime = np.mean(Times)\n",
    "AvgPerUnitTime = len(X)/AvgTime\n",
    "AvgAccuracy = np.mean(Accuracy)\n",
    "\n",
    "print('Average Time {:.3f}'.format(AvgTime))\n",
    "print('Average Rate {:.3f} per second'.format(AvgPerUnitTime))\n",
    "print('Average Accuracy {:.3f}'.format(AvgAccuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time 2.588\n",
      "Average Rate 390.241 per second\n",
      "Average Accuracy 0.976\n"
     ]
    }
   ],
   "source": [
    "modelname = 'OneBidirectionalLSTM_Char_2019-04-15_128_50_150_256_0.5'\n",
    "\n",
    "sequence_length = 50\n",
    "Embedding_Dim = 150\n",
    "Neurons = 256\n",
    "\n",
    "Indx2Word,Word2Indx,Language2Indx,Indx2Language = DownloadWeights(modelname,bucket)\n",
    "Indx2Language['0'] = 'Unknown'\n",
    "model =  ReturnModel(Word2Indx, Language2Indx, sequence_length, Embedding_Dim, Neurons)\n",
    "model.load_weights('Weights.h5')\n",
    "\n",
    "Times = []\n",
    "while len(Times)<10:\n",
    "    t0 = time.time()\n",
    "    Yhat = PredictBatch(X, threshold = 0.3, sequence_length = 50)\n",
    "    t1 = time.time()\n",
    "    Times.append(t1-t0)\n",
    "    \n",
    "Accuracy = []\n",
    "for index,pred in enumerate(Yhat):\n",
    "    if len(pred)==1:\n",
    "        Accuracy.append(pred == Yhat[index])\n",
    "    else:\n",
    "        acc = False\n",
    "        for p in pred:\n",
    "            if p == Yhat[index]:\n",
    "                acc = True\n",
    "        Accuracy.append(acc)\n",
    "\n",
    "AvgTime = np.mean(Times)\n",
    "AvgPerUnitTime = len(X)/AvgTime\n",
    "AvgAccuracy = np.mean(Accuracy)\n",
    "\n",
    "print('Average Time {:.3f}'.format(AvgTime))\n",
    "print('Average Rate {:.3f} per second'.format(AvgPerUnitTime))\n",
    "print('Average Accuracy {:.3f}'.format(AvgAccuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
